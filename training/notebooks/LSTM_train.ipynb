{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786607b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8b82ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1504713ed90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db193280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (1000, 17)\n",
      "Columns: ['timestamp', 'iteration', 'env0', 'raw0', 'env1', 'raw1', 'env2', 'raw2', 'env3', 'raw3', 'thumb_tip', 'thumb_base', 'index', 'middle', 'ring', 'pinky', 'hand_label']\n",
      "\n",
      "First 5 rows:\n",
      "                    timestamp  iteration   env0     raw0      env1     raw1  \\\n",
      "0  2025-10-30T19:57:11.124404          0  495.0  18.0000  494.0000  63.0000   \n",
      "1  2025-10-30T19:57:11.248486          1  494.0  18.0000  494.0000  63.0000   \n",
      "2  2025-10-30T19:57:11.331993          2  494.0  19.0000  495.0000  73.0000   \n",
      "3  2025-10-30T19:57:11.521735          3  494.0   0.7823    0.5059   0.6457   \n",
      "4  2025-10-30T19:57:11.635704          4  495.0  18.0000  493.0000  68.0000   \n",
      "\n",
      "       env2     raw2      env3  raw3 thumb_tip thumb_base   index  middle  \\\n",
      "0  492.0000  59.0000  494.0000    23    0.8696     0.4957  0.6387  0.7544   \n",
      "1  494.0000  38.0000  494.0000    23    0.8148     0.5007  0.6152  0.7285   \n",
      "2  495.0000  60.0000  494.0000    23    0.7807     0.4759  0.6399  0.7707   \n",
      "3    0.7509   0.7863    0.7004  Left       NaN        NaN     NaN     NaN   \n",
      "4  499.0000  64.0000  495.0000    23    0.7991     0.4810  0.6532  0.7819   \n",
      "\n",
      "     ring   pinky hand_label  \n",
      "0  0.7679  0.6821       Left  \n",
      "1  0.7524  0.6913       Left  \n",
      "2  0.7364  0.6758       Left  \n",
      "3     NaN     NaN        NaN  \n",
      "4  0.7567  0.6793       Left  \n",
      "\n",
      "=== DEBUG: Columns after dropping ===\n",
      "['iteration', 'env0', 'raw0', 'env1', 'raw1', 'env2', 'raw2', 'env3', 'raw3', 'thumb_tip', 'thumb_base', 'index', 'middle', 'ring', 'pinky']\n",
      "\n",
      "Data types:\n",
      "iteration       int64\n",
      "env0          float64\n",
      "raw0          float64\n",
      "env1          float64\n",
      "raw1          float64\n",
      "env2          float64\n",
      "raw2          float64\n",
      "env3          float64\n",
      "raw3           object\n",
      "thumb_tip      object\n",
      "thumb_base     object\n",
      "index          object\n",
      "middle         object\n",
      "ring           object\n",
      "pinky          object\n",
      "dtype: object\n",
      "\n",
      "Actually numeric columns: ['iteration', 'env0', 'raw0', 'env1', 'raw1', 'env2', 'raw2', 'env3']\n",
      "\n",
      "Numeric data shape: (1000, 8)\n",
      "First 5 rows of numeric data:\n",
      "[[  0.     495.      18.     494.      63.     492.      59.     494.    ]\n",
      " [  1.     494.      18.     494.      63.     494.      38.     494.    ]\n",
      " [  2.     494.      19.     495.      73.     495.      60.     494.    ]\n",
      " [  3.     494.       0.7823   0.5059   0.6457   0.7509   0.7863   0.7004]\n",
      " [  4.     495.      18.     493.      68.     499.      64.     495.    ]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = os.path.join(\"../../data\", \"integrated_data_20251030_195505.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Try to drop timestamp and hand_label\n",
    "df_numeric = df.drop(columns=['timestamp', 'hand_label'], errors='ignore')\n",
    "\n",
    "# DEBUG: Check what columns remain after dropping\n",
    "print(\"\\n=== DEBUG: Columns after dropping ===\")\n",
    "print(list(df_numeric.columns))\n",
    "print(\"\\nData types:\")\n",
    "print(df_numeric.dtypes)\n",
    "\n",
    "# Only select truly numeric columns\n",
    "numeric_columns = ['iteration', 'env0', 'raw0', 'env1', 'raw1', 'env2', 'raw2', 'env3', 'raw3', \n",
    "                   'thumb_tip', 'thumb_base', 'index', 'middle', 'ring', 'pinky']\n",
    "\n",
    "# Let pandas automatically select only numeric columns\n",
    "data = df_numeric.select_dtypes(include=[np.number]).values.astype(np.float32)\n",
    "print(f\"\\nActually numeric columns: {list(df_numeric.select_dtypes(include=[np.number]).columns)}\")\n",
    "\n",
    "print(f\"\\nNumeric data shape: {data.shape}\")\n",
    "print(\"First 5 rows of numeric data:\")\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fea8ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 1000\n",
      "Train data points: 800\n",
      "Test data points: 200\n",
      "\n",
      "trainX shape: torch.Size([785, 15, 8])\n",
      "trainY shape: torch.Size([785, 0])\n",
      "testX shape: torch.Size([185, 15, 8])\n",
      "testY shape: torch.Size([185, 0])\n"
     ]
    }
   ],
   "source": [
    "sensor_columns = ['iteration', 'env0', 'raw0', 'env1', 'raw1', 'env2', 'raw2', 'env3', 'raw3']\n",
    "finger_columns = ['thumb_tip', 'thumb_base', 'index', 'middle', 'ring', 'pinky']\n",
    "\n",
    "actual_numeric_cols = list(df_numeric.select_dtypes(include=[np.number]).columns)\n",
    "\n",
    "# Find indices based on the actual data array\n",
    "sensor_indices = [actual_numeric_cols.index(col) for col in sensor_columns if col in actual_numeric_cols]\n",
    "finger_indices = [actual_numeric_cols.index(col) for col in finger_columns if col in actual_numeric_cols]\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i + seq_length, :]  \n",
    "        y = data[i + seq_length, :]   \n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Split data into 80% train, 20% test BEFORE creating sequences\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "print(f\"Total data points: {len(data)}\")\n",
    "print(f\"Train data points: {len(train_data)}\")\n",
    "print(f\"Test data points: {len(test_data)}\")\n",
    "\n",
    "# Create sequences separately for train and test\n",
    "seq_length = 15\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "X_test, y_test = create_sequences(test_data, seq_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "trainX = torch.tensor(X_train[:, :, sensor_indices], dtype=torch.float32)\n",
    "trainY = torch.tensor(y_train[:, finger_indices], dtype=torch.float32)\n",
    "testX = torch.tensor(X_test[:, :, sensor_indices], dtype=torch.float32)\n",
    "testY = torch.tensor(y_test[:, finger_indices], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\ntrainX shape: {trainX.shape}\")\n",
    "print(f\"trainY shape: {trainY.shape}\")\n",
    "print(f\"testX shape: {testX.shape}\")\n",
    "print(f\"testY shape: {testY.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e8bb22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        if h0 is None or c0 is None:\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(\n",
    "                0), self.hidden_dim).to(x.device)\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(\n",
    "                0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step\n",
    "        return out, hn, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa451ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_dim=9, hidden_dim=200, layer_dim=4, output_dim=6)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea971aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\Documents\\Cogito\\Grip\\Grip\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:634: UserWarning: Using a target size (torch.Size([785, 0])) that is different to the input size (torch.Size([785, 6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m optimizer.zero_grad()\n\u001b[32m     10\u001b[39m outputs, h0, c0 = model(trainX, h0, c0)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m loss.backward()\n\u001b[32m     14\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tobia\\Documents\\Cogito\\Grip\\Grip\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tobia\\Documents\\Cogito\\Grip\\Grip\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tobia\\Documents\\Cogito\\Grip\\Grip\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:634\u001b[39m, in \u001b[36mMSELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tobia\\Documents\\Cogito\\Grip\\Grip\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3864\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, weight)\u001b[39m\n\u001b[32m   3861\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3862\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3864\u001b[39m expanded_input, expanded_target = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3866\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3867\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight.size() != \u001b[38;5;28minput\u001b[39m.size():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tobia\\Documents\\Cogito\\Grip\\Grip\\.venv\\Lib\\site-packages\\torch\\functional.py:77\u001b[39m, in \u001b[36mbroadcast_tensors\u001b[39m\u001b[34m(*tensors)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "h0, c0 = None, None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs, h0, c0 = model(trainX, h0, c0)\n",
    "\n",
    "    loss = criterion(outputs, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    h0, c0 = h0.detach(), c0.detach()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs, _, _ = model(testX, None, None)\n",
    "        val_loss = criterion(val_outputs, testY)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615dc63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.8)\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cf737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = \"../../models/lstm_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600dff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted, _, _ = model(testX, None, None)\n",
    "\n",
    "\n",
    "predicted = predicted.detach().numpy() \n",
    "original = testY.numpy()  \n",
    "\n",
    "\n",
    "# Create plots for each finger\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, finger in enumerate(finger_columns):\n",
    "    plt.subplot(3, 2, i + 1)  # Create a subplot for each finger\n",
    "    plt.plot(predicted[:, i], label='Predicted', linestyle='--')\n",
    "    plt.plot(original[:, i], label='Original', alpha=0.7)\n",
    "    plt.title(f\"Finger: {finger}\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Grip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
